{
  
    
        "post0": {
            "title": "An Introduction to Version Control with GIT",
            "content": "Using version control tools like GIT is an essential skill for anyone that writes code, and plays an important role in doing research that is reproducable, collaborative, and transparent. Yet, like many other so-called Software Carpentries (SWCs: “basic lab skills for research computing”), GIT isn’t often taught in computer science programs, and rarely so outside them. . I’m passionate about giving students the tools they need to succeed, “filling in the blanks” between what their curricula teaches and what the real world requires. This presentation was created to introduce the concepts of version control as a foundational SWC skill and demonstrate GIT in four key use-cases. . This talk was orignally presented to members of Auburn’s student chapter of INFORMS, mostly graduate students. I hope to one day use it as part of a full course in SWC and other software engineering practices that are neglected but essential to students that will likely spend much of their career doing practical / scientific computing (whether they realize it now, or not). Software, after all, has been “eating the world” for at least 11 years now and shows no signs of retreat. . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it directly on GitHub. .",
            "url": "https://olearydj.github.io/antisimplistic/software%20carpentry/presentation/2022/03/02/IntroToGit.html",
            "relUrl": "/software%20carpentry/presentation/2022/03/02/IntroToGit.html",
            "date": " • Mar 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Graphics and MegaGrants",
            "content": "Modern game engines are flexible, extensible, state of the art tools capable of integrating a wide variety of input types, data sources, and modeling / analysis methods to generate high fidelity imagery on a variety of hardware. . This presentation was originally developed to help raise faculty awareness of the broad applications for these tools in research, education, and enterprise. I also discuss relevant trends and opportunities, and introduce Epic’s Unreal Engine and their MegaGrant program. . This material is adapted from a talk I’ve given a few times at Auburn and, most recently, to the folks at MGMWERX and AUiX at Air University, Maxwell AFB, Montgomery. It is based on latest industry reports and personal experience gained from over 20 years in the industry. . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it directly on GitHub. .",
            "url": "https://olearydj.github.io/antisimplistic/real-time%20graphics/presentation/2021/07/24/rtg-emg.html",
            "relUrl": "/real-time%20graphics/presentation/2021/07/24/rtg-emg.html",
            "date": " • Jul 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Digital Twins of Another Kind",
            "content": "I was finally able to make some time to join the early access program for Epic Games’ amazing new offering, MetaHuman Creator (MHC). First previewed at the 2018 Game Developer Conference, MHC is based on world-leading character creation technology developed by the Serbian company 3Lateral combined with state-of-the-art computer vision and animation technology from England’s Cubic Motion. Both companies have since been acquired by Epic. . . Remarkably, the result of this collaboration is both free and web-based. It was announced in February of this year with this jaw-dropping trailer: . . After spending a few hours with MetaHuman Creator, I agree with Epic on its unique selling points: . Simple, responsive controls make it Easy and Fun to use, and encourage experimentation. It is easy to make a character in 10 minutes, but fun and rewarding enough to spend hours with. | Nearly infinite choices allow for the creation of Hugely Variable characters, as demonstrated by the images in this post, which vary widely in age, gender, skin tone, etc. | Real-world scan data and appropriately constrained adjustments are designed to easily produce diverse, Physically Plausible results. | The output is Real-Time Ready, including everything Unreal needs to build performant apps for a range of hardware: meshes, full facial and body rig, animation controls, materials, and LODs. | . . MHC uses an innovative approach to character modeling that prioritizes quick, amazing results over complete artistic control. It combines parametric controls for blending features from the given base models and adjusting attributes like freckle density, with modeling tools that allow the user to alter the resulting mesh. The latter fall short of sculpting (the term used in MHC) in that you cannot create new geometric features, only adjust what is present in your blended model. . While new characters can be brought to life at the speed of your imagination, the tradeoff described above makes replicating the look of a real person a bit more challenging. The skin around your eyes, for example, is very complex and nuanced. Trying to duplicate a specific example in the wide range of human eye types is difficult in MHC, which only gives you tools for adjusting the location, size, and overall shape of the eye, along with some control over the relative position of the lids. To some extent this can be overcome by blending features from carefully selected base models. . Despite MHC’s constraints and my lack of artistry, I was able to get better-than-Polar-Express results in no time. . My Non-Identical Digital Twin, Don . To experience all of this in your favorite web browser is downright magical. MHC overcomes the limitations of its web-based delivery through the magic of Pixel Streaming. All the heavy lifting is done in an Unreal application running on a remote server, where the GPU-rendered scene is encoded as a browswer-friendly media stream. This approach eliminates most hardware and software requirements, and ensures that all users have a similarly amazing experience. . MetaHuman Creator is still an “early access” preview of what’s to come. It would benefit from a greater variety of hair styles, clothing, and body types, as well as continued development of the parametric controls and modeling tools. Even with the current “limits” it is a remarkably capable tool, and an exciting glimpse into the bright future of real-time graphics. For quickly creating a very diverse range of fantastic looking human characters that are real-time ready, it is hard to imagine a better debut. Yet I’m sure that Epic’s world-class team is just getting started, and I look forward to being equally inspired and awe-struck by future releases. . .",
            "url": "https://olearydj.github.io/antisimplistic/real-time%20graphics/2021/06/27/MetaHumanCreator.html",
            "relUrl": "/real-time%20graphics/2021/06/27/MetaHumanCreator.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Design and Innovation Director Presentation",
            "content": "In the Spring of 2021, Auburn posted an opening for “Design and Innovation Director.” This was an opportunity to lead the development and direction of the College of Engineering’s brand new 11,000sf makerspace. I had been an enthusiastic supporter of the makerspace since its soft opening earlier that year, and was the first instructor to schedule regular classes in it. I applied and was ultimately named one of three finalists for the position. . This presentation was developed to share my vision for the space with the hiring committee and invited guests. As usual, the process of creating it was educational and rewarding in itself. I strongly believe in making as a way to develop skills, confidence, and understanding in engineers. . Though the position was ultimately given to an incumbent with a more traditional academic background, I was thrilled to make it to the final stage of this national search, and learned a lot along the way. I continue to utilize the makerspace in my teaching and enthusiastically support the new Director’s vision. . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it directly on GitHub. .",
            "url": "https://olearydj.github.io/antisimplistic/makerspace/presentation/2021/06/23/did-present.html",
            "relUrl": "/makerspace/presentation/2021/06/23/did-present.html",
            "date": " • Jun 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "BET 3510 Spring 2021 Course Highlights",
            "content": "The Spring 2021 semester has ended and with it my first semester as Instructor of Record for BUSI / ENGR 3510, Introduction to Business and Engineering. This course follows 3520, Integrating Business and Engineering Theories with Practice (don’t ask me why the names and numbers are in reverse order), and concludes the first year of Auburn’s Business-Engineering-Technology (BET) minor. . Using 100 of more than 400 slides presented, this deck highlights the key topics I cover in the updated Spring offering. Six guest speakers, eight makerspace lab sessions, and two site visits support, extend, apply, and authenticate those lectures. . You can find a similar deck for the prior class here: BET 3520 Fall 2020 Course Summary . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it directly on GitHub. .",
            "url": "https://olearydj.github.io/antisimplistic/teaching/presentation/2021/05/05/bet3510-spring2021.html",
            "relUrl": "/teaching/presentation/2021/05/05/bet3510-spring2021.html",
            "date": " • May 5, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "An Introduction to Publishing with R Markdown",
            "content": "I recently completed a course that used R for various statistical and machine learning methods. In a separate article I have discussed the course and my semester project, Text Sentiment Analysis with R. Another is coming soon with R Markdown tips and tricks I learned along the way. But first some background is in order… . A Case for Reproducible Workflows . In this article I’ll provide a simple introduction to the R Markdown for the uninitiated. I consider this approach, a form of single source publishing, a vital component of reproducible computational research, and an essential skill. The following short video dramatization of real-life events provides a motivating example that we can all relate to… (Link opens in YouTube) . . The video ends with the glimpse of a better future, a hopeful vision of reproducible workflows powered by source control (git) and R Markdown. It is within reach! The tools are built into RStudio and the essentials are easy to grasp and apply. The short time it takes to begin using this method will pay generous dividends ever after. . As this post grew and grew, I felt like the initial promise of a solution “within reach” seemed less and less credible. Perhaps after writing and digesting this I will be able to return to the topic and create the TL;DR version… . What is R Markdown? . R Markdown was introduced in 2012 by Yihui Xie, who has authored many of the most important packages in this space, including {knitr}. In his book, R Markdown: The Definitive Guide, Xie describes R Markdown as “an authoring framework for data science,” in which a single .Rmd file is used to “save and execute code, and generate high quality reports” automatically. A very wide range of document types and formats are supported, including documents and presentations in HTML, PDF, Word, Markdown, Powerpoint, LaTeX, and more. Homework assignments, journal articles, full-length books, formal reports, presentations, web pages, and interactive dashboards are just some of the possibilities he describes, with examples of each. To say it is a flexible and dynamic ecosystem is an understatement. . R Markdown Syntax . Anyone that has worked with R Markdown’s Python equivalent, Jupyter Notebooks, will find the following familiar. You may also be interested in my article Exploring Jupyter Notebook-Based Research. . R Markdown files consist of the following elements: . A metadata header that sets various output options, using the YAML syntax | Body text that is formatted in Markdown | Interspersed code chunks that are set off by three backticks and a curly braced block that specifies the language used and sets chunk options | . R Markdown also allows users to embed code expressions in the prose by enclosing them in single backticks. . Sample Code and Output . A minimal .Rmd example is included below. This is based on the Xie’s sample, which I’ve modified to demonstrate all of the elements mentioned above. . 01: 02: title: &quot;Hello R Markdown&quot; 03: author: &quot;Awesome Me&quot; 04: output: html_document 05: 06: 07: This is a paragraph in an _R Markdown_ document. 08: Below is a code chunk: 09: 10: {r fit-plot, echo=TRUE} 11: fit = lm(dist ~ speed, data = cars) 12: b = coef(fit) 13: plot(cars) 14: abline(fit) 15: 16: 17: The slope of the regression is `r round(b[2], digits = 3)`. . Let’s take a moment to walk through this, line by line. . Lines 01 through 05 are the YAML header (aka frontmatter), delimited above and below by lines containing three dashes. Here, we’ve specified the document’s title and author, which are used to generate a header in the rendered output. Other options like date and subtitle are also available. Also specified is the default output type, html_document. This section is actually optional, though rarely omitted. When it is, documents are rendered to HTML. More on YAML after the more pressing matters that follow. | Lines 07 and 08 are the body text (aka prose or narrative). The bit _R Markdown_ is an example of Markdown syntax, causing the text enclosed in underbar characters to be rendered in italics. A few quirks aside (e.g. newlines are considered spaces), the rest of the Markdown syntax is similarly straightforward, but surprisingly capable. | Lines 10 through 15 are an R code chunk, delimited by three backticks () Line 10 includes {r fit-plot, echo=TRUE}, which specifies the language for this code block (R) and the chunk name (fit-plot). It also sets a chunk option echo=TRUE, specifying that both the code and result will be included in the rendered output. | Lines 11 through 14 are standard R code | Line 15 closes the code chunk | . | Line 17 is body text with an inline R expression. This expression will be evaluated when the document is rendered and the result will be included in the resulting output. | . Rendering this code to an HTML file via the Knit button results in the following: . . Creating and Running R Markdown . In RStudio, .Rmd files can be created from the File menu, by selecting New File... and then R Notebook. A template .Rmd will be opened, containing a YAML header, some body text, and a few code chunks to get you started. . Add chunks either by typing the backtick and curly brace syntax directly, using Insert on the editor toolbar, or with the keyboard shortcut Cmd/Ctrl + Alt + I. Pro-tip: this can also be used to :boom:SPLIT existing chunks!:boom: . The difference between an .Rmd file and an R Notebook is subtle but important. In the context of RStudio, R Notebooks should be thought of as a particular interface for .Rmd files. This interface is triggered whenever html_notebook is the default (first or only) output specified, which is the case for any newly created R Notebook. . Note the important distinction between the output types html_notebook, which enables the R Notebook Preview features, and html_document, which specifies a Knit output target. . The R Notebook interface provides three primary benefits. . Code chunks can be executed independently and interactively. Click the green triangle button on the chunk toolbar or use Cmd/Ctrl + Shift + Enter to run the current chunk. | Use Cmd/Ctrl + Enter to run the current/selected line/lines. | Additional options including Run All and Run All Chunks Above are available from the Run menu in the editor toolbar. | . | The output of each code chunk is displayed inline, just below the input. Chunk options are used control the contents and style of the output of individual chunks. For example, you can choose to hide the text output and control the aspect ratio of a figure generated. | The output cells can be cleared or collapsed using controls in the upper right corner of the output or via the gear menu in the editor toolbar, next to the Preview button. | . | Automatically updated HTML Previews! Preview replaces the Knit button on the editor toolbar when using the R Notebook interface. Clicking it displays an HTML preview in the Viewer pane. | This last point bears further discussion… | . | Ordinary R Markdown documents are “knitted,” but notebooks are “previewed.” While the notebook preview looks similar to a rendered R Markdown document, the notebook preview does not execute any of your R code chunks. It simply shows you a rendered copy of the Markdown output of your document along with the most recent chunk output. This preview is generated automatically whenever you save the notebook. . In short, create a new R Notebook, click Preview, make changes to the Notebook, save the changes, and violà! Finish your work, using the automatic Previews to guide your edits. Then Knit to the desired final format(s) using the drop down list on the Preview button. . . This is a huge time-saver. When Knit is used to render final output the entire Notebook is run from scratch. Depending on its contents this can take a while, but even simple Notebooks take several seconds to process. By contrast, the Preview HTML is generated incrementally, as each change is made, and updated instantly with each save. . The result is not usually a true WYSIWYG experience due to differences between the way HTML and your final output format is rendered (e.g., HTML doesn’t have page breaks), but saves countless round-trips through the Knit-check-edit process. An added benefit of this approach is accelerated learning, greater willingness to experiment, etc., all engendered by the reduced friction that comes with near-instantaneous feedback. . Rendering Process . Clicking the Knit button kicks off the following process: . . In a nutshell, R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on). - Xie, Preface . While it is not typically necessary to understand the details of this process it it useful to know that both knitr and Pandoc are involved. It is, however, critical to understand the following: . Under the hood, RStudio calls the function rmarkdown::render() to render the document in a new R session. Please note the emphasis here, which often confuses R Markdown users. Rendering an Rmd document in a new R session means that none of the objects in your current R session (e.g., those you created in your R console) are available to that session. - Xie, Compile an R Markdown document . In other words your .Rmd must run start to finish, without errors, from a clean environment. If, for example, the code relies on variables created or packages loaded via the console during your interactive session, the Knit will fail. This requirement goes a long way to ensuring that others, given the same file, will get the same results. For more details, follow the link in the quote above. . Troubleshooting . To confirm that your .Rmd runs cleanly, perform the following steps: . From RStudio’s Session menu, choose Clear Workspace.... In the dialog box that follows, confirm that the option “Include hidden objects” is ticked and click Yes. This clears your environment. | Return to the Session menu and choose Restart R and Run All Chunks. This starts a fresh R session and runs the entire notebook, start to finish. | As an alternative to the first step you can include rm(list=ls()) at the top of your code. I believe this achieves an equivalent result, removing all objects in the current workspace. With that addition, simply use Restart R and Run All Chunks to reinitialize the R kernel and run your code, which will automatically clear the environment. . Finally, if Knit or Run All Chunks is failing in a way you don’t understand, step through the code. After clearing the workspace choose Restart R and Clear Output in the Session menu. Now use the methods described in [Running Files], above, to run each chunk until you find the problem. To isolate issues within a problematic chunk, break it into multiple chunks and/or run it line by line. . Any missing dependencies that would halt the Knit process should be identified by these methods. . Conclusion . It’s (way past?) time to draw this to a close. I hope that it has given you a grasp, or at least a taste, of: . The value of reproducible research | The important role that tools like R Markdown and R Notebook play in supporting reproducible research by combining prose, code, output, and supporting materials in a single document | The syntaxes used in .Rmd files, including YAML, Markdown, and code chunks | The concept that R Notebooks provide a way to interact with .Rmd files | How to use Preview mode in R Notebooks to efficiently create great looking reports and improve your results | How to render final output using Knit and how to troubleshoot problems that may arise | The fundamental mechanics involved so you can at least ask informed questions when something isn’t working or you need to learn more | Links to valuable resources when that time comes | . If you learned half as much reading this as I learned creating it then we’re both doing well! . Further Reading . You could write a book about this. In fact, people have. Several… . This article relied heavily on Yihui Xie’s R Markdown: The Definitive Guide, specific sections of which have been linked throughout. In addition, it is hard to go wrong with any of his books, most of which are available online for free, and in print: . Xie, Yihui, Alison Presmanes Hill, et al. Blogdown: Creating Websites with R Markdown. CRC Press, 2017. Google Books, https://bookdown.org/yihui/blogdown/. | Xie, Yihui. Bookdown: Authoring Books and Technical Documents with R Markdown. CRC Press, 2016. Google Books, https://bookdown.org/yihui/bookdown/. | —. Dynamic Documents with R and Knitr, Second Edition. Taylor &amp; Francis, 2015. | Xie, Yihui, J. J. Allaire, et al. R Markdown: The Definitive Guide. CRC Press, 2018. Google Books, https://bookdown.org/yihui/rmarkdown/. | Xie, Yihui, Christophe Dervieux, et al. R Markdown Cookbook. CRC Press, 2020. Google Books, https://bookdown.org/yihui/rmarkdown-cookbook/. | . You might also check out Hadley Wickham and Garrett Grolemund’s classic, R for Data Science, especially the last section, Communicate, chapters 26 through 30. . Wickham, Hadley, and Garrett Grolemund. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc., 2016. Google Books, https://r4ds.had.co.nz/index.html. | . Finally, I found this wayward web page had some good tidbits. . R Markdown Basics. http://stats.idre.ucla.edu/stat/data/rmarkdown/rmarkdown_seminar_flat.html. Accessed 9 Dec. 2020. | .",
            "url": "https://olearydj.github.io/antisimplistic/r/2020/12/08/rmd-intro.html",
            "relUrl": "/r/2020/12/08/rmd-intro.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Recent R Learnings",
            "content": "I recently completed a course that used R for various statistical and machine learning methods. In separate articles I discussed the course and my semester project, Text Sentiment Analysis with R, and provided an introduction to R Markdown. . Here I’ll focus on how I used R Studio’s markdown-based publication tools to integrate my analysis and documentation efforts and render the final results in a variety of formats. . Course Application . Throughout the course I used R Notebooks to combine my prose, code, results, and supporting materials into a single file and render the resulting report directly to PDF. . I started rendering to DOC, which I then converted to PDFs. Spare yourself the pain! Going direct to PDF was as easy as installing TinyTeX. . Tips and Tricks . Keyboard Shortcuts . The keyboard shortcut for inserting an R code chunk is Cmd + Option + I on Mac (Ctrl + Alt + I on Windows). This will insert a new chunk at the cursor position or :boom: split the current chunk! :boom: :astonished: . Converting Between R and R Markdown . Convert your R Markdown file to a standard R file, automatically stripping out the prose and results with knitr::purl(&quot;file.Rmd&quot;, documentation = 1). You can extract just the code, code and chunk headers (the default), or code with text chunks as roxygen comments with documentation parameters of 0, 1, and 2, respectively. . The opposite is also possible using spin to convert R files into R Markdown. Here roxygen comments are used to add markdown (e.g. #&#39; this is *markdown*) or code chunk options (e.g. #+ load-libraries, include=FALSE). For example, given the following R file with a mix of comment styles: . #&#39; ## Initialize #&#39; *Fun* with `mtcars`! #+ summary-stats, echo=FALSE # Summarize mpg summary(mtcars$mpg) . knitr::spin(&quot;spin-me.R&quot;, knit = FALSE) results in the file spin-me.Rmd, with two chunks. The first is markdown with a level 2 header and some bold formatted text. The second is an R code chunk named summary-stats with echo=FALSE that includes a comment: . ## Initialize *Fun* with `mtcars`! {r summary-stats, echo=FALSE} # Summarize mpg summary(mtcars$mpg) . If you are only interested in publishing your R files, simply omit the knit = FALSE parameter from spin to go directly to the final rendered output, e.g. md, pdf, etc. . To evaluate expressions included in body of your text, simply place the cursor in the inline chunk and press Cmd/Ctrl + Enter. The expression will be evaluated in the console, with the results displayed there and in a small pop-up window near the code. . More Details . Note that YAML is sensitive to white space, which is part of the syntax, and doesn’t allow tabs. You can mostly learn this as you need it, referring to this description by Xie, or the YAML and output related sections of this article. . ?rmarkdown::html_document for details on options . Pandoc flavor . Code output is controlled by options set in each chunk’s header. . Twitter . I found several of these on Brendan Cullen’s recent twitter thread: . #rstats friends: what are your favorite “lesser known” tips &amp; tricks for using R Markdown? e.g. any feature(s) (big or small) you’ve picked up at some point along your #rmarkdown journey that you now use on a regular basis and wish you knew earlier? 👀👂 . &mdash; Brendan Cullen (@_bcullen) December 1, 2020",
            "url": "https://olearydj.github.io/antisimplistic/r/2020/12/08/r-tips-and-tricks.html",
            "relUrl": "/r/2020/12/08/r-tips-and-tricks.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Text Sentiment Analysis with R",
            "content": "Introduction . This semester I took INSY 7130 - Data Mining Techniques and Applications for Operations, one of four classes required for the Modeling and Data Analytics for Operations graduate certificate offered by the Department of Industrial and Systems Engineering at Auburn University. I would describe this course as an introductory survey of classic methods. A lot of focus was put on clustering, including hierarchical, k-means, fuzzy k-means, BIRCH, and Kohonen (self-organizing maps) algorithms. Support Vector Machines and Backpropagation Neural Networks were also covered, along with smoothing, time series, and association rules. Most concepts were covered in a single lecture, followed by a second workshop-styled session to demonstrate an R implementation. . The semester project was very open-ended, allowing students to pick a topic of interest and apply suitable methods to explore and make inferences about the data. We were required to us at least one method from the class. I used the following criteria to guide my topic selection process: . Supervised learning using SVM and/or BPNN to meet the project requirement | Something with helpful tidyverse / tidymodels examples that I could use as a framework, allowing this project to also support my self-directed learning | Anything but tabular data! | Something fun and different | . I quickly focused my search on the TidyTuesday project archive and the Animal Crossing: New Horizons exercise, for which Julia Silge documented an excellent solution that I could borrow heavily from. This ticked all the boxes. . TL;DR . The following slides summarize the project and were adapted from a shorter presentation that was given in class. The full report follows for those interested in more detail. . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it on GitHub. . Project . The goal of this project is to perform text sentiment analysis on a data set of user reviews for the Nintendo Switch game Animal Crossing: New Horizons (ACNH), a “life simulation video game developed and published by Nintendo for the Nintendo Switch,” (Wikipedia). . . From their starting point on a deserted island, ACNH players explore and customize their island, gather and craft items, and catch insects and fish. All of this activity (and much more) plays out in real calendar time as players invest many hours developing their islands into thriving communities of anthropomorphic animals. . In less than 6 months following its March 2020 release, ACNH sold over 22 million units worldwide. It became something of a cultural phenomena in the process, complete with celebrity sightings, grandmothers logging 3,500+ hours of play time, and a thriving in-game currency market based on turnips. . While ACNH provides an interesting, lighthearted context for this project, very little domain knowledge is required, and the problem and methods associated with natural language processing and sentiment analysis are broadly applicable. The roots of this field can be traced back to public opinion analysis in the early 20th century. It wasn’t until the mid 2000’s that modern data mining methods made the problem tractable, with 99% of related papers appearing after 2004. Since then, over 7,000 papers have been published on the topic, making it one of the fastest growing research areas in data science (Mäntylä et al.). . “Literature Review” . This project is based on a TidyTuesday exercise. TidyTuesday is a “social data project in R” that gives R users of all levels an opportunity to “apply [their] R skills, get feedback, explore other’s work, and connect with the greater #RStats community” every week. Past data sets have featured everything from Ikea Furniture to Global Crop Yields. . The original TidyTuesday Animal Crossing exercise is linked here. While completing this project I relied on a number of responses to that exercise, including Julia Silge’s blog post. . The modeling and analysis for this project was performed in R using a workflow based on the tidyverse and tidymodels library collections, employing best practices advocated in those communities. This document was generated directly from the R Markdown file (provided separately), which includes all code. . In addition to the aformentioned blog page, I referred to several other resources created by Ms. Silge and collaborators: . Supervised Machine Learning for Text Analysis in R, an online book authored in R Markdown | Predictive Modeling with Text Using Tidy Data Principles, a tutorial presentation supporting the previous book | Tidy Modeling in R, another online book | Supervised Machine Learning Case Studies in R, a short online course | . Detailed Problem Description . The user_review data provided for this exercise was originally scraped from the popular meta-review site Metacritic. Each of the 2,999 observations has four features: grade, user_name, text, and date. The grade feature is the review score, an integer in the range [0, 10], and text is the written feedback provided by each user. The other fields are self-explanatory. A summary of the data structure is provided below: . ## Rows: 2,999 ## Columns: 4 ## $ grade &lt;dbl&gt; 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, … ## $ user_name &lt;chr&gt; &quot;mds27272&quot;, &quot;lolo2178&quot;, &quot;Roachant&quot;, &quot;Houndf&quot;, &quot;Profess… ## $ text &lt;chr&gt; &quot;My gf started playing before me. No option to create … ## $ date &lt;date&gt; 2020-03-20, 2020-03-20, 2020-03-20, 2020-03-20, 2020-… . My goal is to build a model that can interpret each reviewer’s sentiment for the game based on the review text. This is a supervised learning problem where grade is used to create a binary label for each review. The underlying assumption is that the score assigned by each user is consistent with the text of their review. The resulting model can be used to categorize user sentiment for unlabled review data. . Development . A full end-to-end analysis follows, including: . Initial data cleaning and exploration | Implement a strategy for model assessment using cross-validation | Build a series of models of increasing complexity | Explore the impact of typical text analysis methods on model accuracy | Rigorously explore the hyperparameter space using grid-based search methods | Select the best model based on appropriate performance metrics | Fit the final model to the full training set &amp; evaluate its performance on test data | . Four different models are built and evaluated in this manner, as described in the Experiments section, below. . Data Prep and Exploration . Before any model is built it is important to explore the data set. I began by creating a histogram to visualize the distribution of user review scores. . . It would be reasonable to expect a gaussian-like distribution here, with a rounded peak somewhere between 3 and 7, tailing off on either side. Instead, we see nearly the opposite, with peaks at 0 and 10, and a shallow u-shaped distribution between. In the world of online reviews, especially for games, this sadly predictable behavior. Here some domain expertise is helpful… . Review Bombing . Despite ACNH’s metacritic score of 90% (making it one of the highest rated games of 2020 in the eyes of professional critics), its user reviews average just 55%. Players critical of the limits placed on all but the first player to start the game have resorted to review bombing: . “an Internet phenomenon in which large groups of people leave negative user reviews online for a published work, most commonly a video game or a theatrical film, in an attempt to harm the sales or popularity of a product, particularly to draw attention to an issue with the product or its vendor,” - Wikipedia. . The graph above clearly depicts this adolescent behavior, where the “haters” and “fanboys” have squared off to show their ire and support, respectively. This is something to keep in mind throughout the project. . Cleaning and Feature Engineering . For the purposes of this project it is also important to have a good sense of what is found in the review text field. Samples of review text were pulled for both positive (grade &gt; 8) and negative (grade &lt; 3) observations and inspected. In doing so I noticed that longer entries tended to include repeated text and end with the word “Expand.” These problems were likely introduced during the scraping process. I also noticed that some entries are not in English. . After some light initial cleanup of the text field, a new rating column was added to label each review based on the grade. In order to use binary classification methods, rating was set to “good” for values greater than 5.0 and “bad” otherwise. This cutoff point was chosen based on the average user review of 5.5, but it also seems natural. The remaining cleanup of text data is done as part of the Tokenization process, described below. . Final Exploration . Two final plots were created to complete my exploration. First, the number of words per review was visualized using a histogram. The discontinuity in the resulting graph gives further evidence of problems with the scraped data. Otherwise, this result is as expected, with most reviews being relatively short (less than 75 words), but some users have submitted nearly 1,000 words! The red vertical line shows the average of 120.95 words per review. . Next, I looked for obvious relationships between grade and review length using a box and whiskers plot. The number of outliers increases with more extreme reviews, which seems to follow my intuition that reviewers scoring the game very high or very low are more prone to support their scores with extended rants / raves. . . Train-Test Split . Lastly, the data was split into training and testing sets using the customary 75/25 split. Training data will be used to fit the model and tune the hyperparameters. Test data is reserved for final evaluation of the best model to estimate its performance on new, unseen data. The split was stratified by rating to ensure that the training and test sets were balanced on the response variable. The splits are summarized below. . ## rating count percent source ## 1 bad 1369 60.84 train ## 2 good 881 39.16 train ## 3 bad 456 60.88 test ## 4 good 293 39.12 test . Experiments . Null, Naive Bayes, Support Vector Machine, and Logistic (Lasso) Regression models are built and evaluated on both training and test data. The Null model is used to establish baseline performance metrics. The others were chosen for their performance on sparse data, a key characteristic of natural language problems that results from tokenization of the input data. . Tokenization . Tokenization is the process of converting the input text data into a format suitable for training ML algorithms. While many approaches are used, the method I used covers the essential steps. It begins with splitting the raw strings into tokens based on a combination of whitespace, punctuation, and other factors. I will use word tokens, as is the most common practice, but tokens can be characters, groups of words (n-grams), etc. . The resulting token list is filtered down using a list of stop words to remove elements with little predictive value. The reduced list is then sorted by frequency and the top 500 tokens are converted to “term frequency-inverse document frequency”, or TFIDF format. According to the documentation, TFIDF is the product of term frequency, the number of times each token appears in each observation, and the inverse document frequency, a measure of how informative a word is based on its rarity). Together these assess the overall value of each token. Finally, the TFIDF values are normalized. . These steps are implemented in the tidymodels::recipe defined below: . review_rec &lt;- recipe(rating ~ text, data = review_train) %&gt;% step_tokenize(text) %&gt;% step_stopwords(text) %&gt;% step_tokenfilter(text, max_tokens = 500) %&gt;% step_tfidf(text) %&gt;% step_normalize(all_predictors()) . This recipe of preprocessing steps will be shared by all model worflows. When applied to the training data, the result is a table with 2,250 rows (one per observation), and 501 columns (one per token, plus the response rating). An extract is shown below for the tokens “access”, “account”, and “accounts”. . ## rating tfidf_text_access tfidf_text_account tfidf_text_accounts ## 1 bad -0.1447401 -0.2129978 -0.1330606 . Method 0 - Null Model Baseline . To establish a performance baseline, a Null Model was first constructed. This trivial case predicts the majority class for all observations. Based on the prior stratification statistics, we should expect 60.8% accuracy classifying as bad. . The tidymodels framework was used to define a model specification and workflow (including the Tokenization recipe described above), fit the training data and generate predictions using 10-fold cross-validation, collect the results, and generate the following confusion matrix. . ## Truth ## Prediction bad good ## bad 137 89 ## good 0 0 . As expected, all predictions use the majority class, bad, and the resulting accuracy is 60.8%. Note that only a single fold’s worth of predictions are included in this result. Using the same model to make predictions for all test data results in the following confusion matrix. . ## Truth ## Prediction bad good ## bad 456 293 ## good 0 0 . Here again the accuracy is 60.9%. The train and test accuracy match in this case because the split was stratified on the response. The Null Model is more accurate than the proverbial coin flip (50%), providing a useful point of comparison for the other models. . Method 1 - Naive Bayes . The first non-trivial model is a Naive Bayes classifier, which uses conditional probabilities to make predictions based on per-class statistics. This method is commonly used for text classification because it is fast, simple, and well-suited to sparse, high-dimensional data. I expect it to perform better than the baseline, but not as well as more sophisticated models to follow. . The same method was used to train the model, generate predictions, and collect metrics. Here we generate a Receiver Operator Characteristics (ROC) curve based on the results of each cross-validation fold. . . This shows the true positive rate (aka recall, aka sensitivity) on the vertical axis vs. the false positive rate (aka (1- text{specificity})). While the end points are pinned at [0, 0] and [1, 1], ideal curves are close to the top left, where true positives are common and false positives are rare. The area under the ROC curve, or ROC-AUC value, is a useful summary statistic. Values of ROC-AUC less than 0.5 perform worse than a random guess, and the max value is 1.0. . The corresponding confusion matrix for a representative cv fold is given below. The average cross-validation accuracy is 74.0%. . ## Truth ## Prediction bad good ## bad 124 48 ## good 13 41 . This is an improvement over the Null Model, but it is clearly better at predicting bad reviews than good ones. When used to generate predictions for all test data, the confusion matrix looks similar. . ## Truth ## Prediction bad good ## bad 418 156 ## good 38 137 . As expected, the Naive Bayes model performs better than the baseline but there is still room for improvement. The test set accuracy of 74.1% suggests that we are not overfitting this model. . Method 2 - Support Vector Machine . The Support Vector Machine model is a significantly more sophisticated method. As discussed in class this semester, SVMs work by finding hyperplanes that optimize class boundaries. A radial basis function (RBF) kernel is used to allow for nonlinearity, where the hyperparameter ( sigma) controls model complexity. . The processed used in this case is the same as above, with one notable addition. A range of values for rbf_sigma are tested using grid_regular() to find the optimal model. This results in 22,500 predictions: 10 cross-validation folds x 10 levels for sigma x 2,250 training observations. Unfortunately, the SVM solver does not return prediction probabilities that are required to build a ROC curve. The plot below shows accuracy, sensitivity, and specificity as a function of ( sigma). . . These curves can be used to select the best value of rbf_sigma for our purposes. The top three models, based on training accuracy, are given below. . # best accuracy for the training data show_best(svm_grid, &quot;accuracy&quot;)[1:3, ] . ## # A tibble: 3 x 7 ## rbf_sigma .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 accuracy binary 0.867 10 0.00650 Preprocessor1_Model10 ## 2 0.0774 accuracy binary 0.606 10 0.0136 Preprocessor1_Model09 ## 3 0.00599 accuracy binary 0.393 10 0.000786 Preprocessor1_Model08 . From this we see that mean accuracy is maximized when ( sigma = 1.0). Honestly, this result seems fishy to me, but I was unable to find fault with the method. Using the optimal parameter value the model was fit using all training data and tthe following predictions were generated for the test set. . ## Truth ## Prediction bad good ## bad 409 47 ## good 47 246 . This looks quite good. Accuracy is 87.4% and the positive and negative predictive value is more balanced. It is easy to see why SVMs are a go-to tool for natural language processing. . Method 3 - Logistic Regression . The fourth and final method uses Logistic Regression with L1 regularization (i.e. lasso) to control model complexity. A range of values are tested for the penalty parameter, adjusting the amount of regularization applied. The resulting ROC curve is plotted below. . . These curves bend closer to the top left corner than the Naive Bayes model, supporting our expectation that this is a better model. As with the SVM model, key metrics are plotted vs the hyperparameter of interest, and the top three models are listed by accuracy. . . ## # A tibble: 3 x 7 ## penalty .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.00599 accuracy binary 0.884 10 0.00534 Preprocessor1_Model08 ## 2 0.000464 accuracy binary 0.839 10 0.00800 Preprocessor1_Model07 ## 3 0.0000359 accuracy binary 0.804 10 0.00931 Preprocessor1_Model06 . The top model listed, with accuracy 88.4, has a ROC AUC score of 0.94. Applying the optimal model to the test set yields the following confusion matrix. . ## Truth ## Prediction bad good ## bad 417 52 ## good 39 241 . The final accuracy here is 87.9%, slightly better than what was obtained with the SVM model. . Summarize Analysis . Beginning with the trivial Null Model, the increasing complexity of each subsequent method resulted in improved results. The accuracy on training and test data is summarized below for all models. . ## Model TrnAcc TstAcc ## 1 Null 60.80 60.90 ## 2 NB 74.00 74.10 ## 3 SVM 86.71 87.40 ## 4 LR 88.36 87.90 . With its marginally improved accuracy and reasonably balanced positive / negative predictive power the Logistic Regression model seems the best choice in this exercise. My concerns about the SVM’s grid search results give me even more confidence in this choice. . Further Exploration . As a final step I adopted the following code from Julia Silge’s ACNH blog post. It uses the vip package to assess the relative importance of predictor variables. This provides valuable insights into both how the model works and the concepts that drive user sentiment about the game. . . From this we can see that bad reviews are driven by the words “greedy,” “copies,” and “second,” all of which relate to the outrage over game limitations we discussed before. On the other hand, good reviews focus on the game being “great,” “beautiful,” and “charming,” among other similarly positive descriptors. . Conclusion . Text sentiment analysis is a valuable tool with many applications. This project gave me a first taste of the process, methods, and tools. It is a deep, fast moving topic that draws from diverse knowledge areas, making it a challenge to master despite the intuitive concepts. . Now that I have a broad understanding, future efforts to more deeply understand best practices for the front-end methods, including preprocessing, feature engineering, stop words, and tokenization, would likely yield the most dramatic improvements. Some specific improvements that I identified during this project include: . Make it multi-class with 3-5 classifications, e.g. good / ok / bad | Improve the quality of the data to minize / eliminate scraping errors | Experiment with different stop word lists, which tend to be domain specific | Experiment with the tokenizing approach, trying n-grams of various length | Use clustering and/or association rules to identify groups of words that relate to user sentiment | Expand the tuning grid parameters, including max_tokens and other preprocessing variables | Try neural net and/or deep neural net models, which are very popular for NLP | . In short, I’ve only scratched the surface of what is possible with this project. It has been a valuable effort and great capstone to the class. . References . Mäntylä, Mika V., et al. “The Evolution of Sentiment Analysis—A Review of Research Topics, Venues, and Top Cited Papers.” Computer Science Review, vol. 27, Feb. 2018, pp. 16–32, doi:10.1016/j.cosrev.2017.10.002. | Silge, Julia. Supervised Machine Learning Case Studies in R! · A Free Interactive Course. https://supervised-ml-course.netlify.app/. Accessed 21 Oct. 2020. | —. Supervised Machine Learning for Text Analysis in R. https://smltar.com/. Accessed 21 Oct. 2020. | —. Tidy Modeling with R. https://www.tmwr.org/. Accessed 21 Oct. 2020. | Silge, Julia, and Emil Hvitfeldt. Predictive Modeling with Text Using Tidy Data Principles. https://emilhvitfeldt.github.io/useR2020-text-modeling-tutorial/. Accessed 2 Dec. 2020. |",
            "url": "https://olearydj.github.io/antisimplistic/r/portfolio/2020/12/07/acnh-reviews-project.html",
            "relUrl": "/r/portfolio/2020/12/07/acnh-reviews-project.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Exploring Jupyter Notebook-Based Research",
            "content": "Much of my life has been centered around software. In middle school I was writing vertical blank interrupts in assembly language for the 6502 in my Atari 800. In 1994 I co-founded the first of two software development companies that I’ve run ever since. Despite all that, I’ve never considered myself a “software guy.” The traditional pattern of write code, compile, check is… well, hard. And discouraging. I have tremendous respect for folks that have mastered this, and consider myself very fortunate to have employed many great, and a few truly exceptional software engineers. . But for me, there had to be a better way. Upon returning to graduate school for Dan 2.0, I found it. Python and Jupyter Notebooks (JNs). The combination of Python’s interpreted nature and the way that JNs combine code, output, and text provided an approach that was much more natural and welcoming to me. Over time, it allowed me to build enough skill and confidence “hacking away” at data science projects to begin exploring software engineering and carpentry. . To that end, I’ve recently spent a lot of time reading about workflows built around notebooks, especially those that facilitate good software engineering practices, single-source publishing, integration, and deployment. Below I’ve shared some of the most insightful links I encountered in that search, along with short summaries and relevant background information. As my PhD focus transitions from classes to research, my goal is to leverage some of these methods to improve the efficiency, quality, and reproducibility of my work while making it easier to publish in a variety of ways. . nbdev: use Jupyter Notebooks for everything: this article first opened my eyes to very interesting things going on in the JN ecosystem, leading to many of the articles linked below. nbdev is a “highly opinionated” Python programming environment that supports software engineering best practices (automated documentation, package creation, testing, continuous integration, source control, and more) for JNs. nbdev was created by the team that created the deep learning PyTorch front-end fastai and the JN-based blogging tool fastpages, which this page is built with. Not at all coincidentally, fastpages is powered, at least in part, by nbdev. All of this was developed by the article’s author Jeremy Howard, former President and Chief Data Scientist of Kaggle, and Sylvain Gugger, now at Hugging Face. Together, they co-authored Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD. I am working with nbdev and will share my experience in future updates. | How to use Jupyter Notebooks in 2020: the first of an insightful three-part series on the JN ecosystem. In it, author Lj Miranda covers the evolution of methods of practice in the data science landscape, current tools and best practices for notebook-based development workflows, and thoughts on the future. I found the second part particularly useful, with lots of technical detail. | Beyond Interactive: Notebook Innovation at Netflix: continuing with the JN theme, this article discusses how “notebooks are the most popular tool for working with data” at Netflix. It describes the workflow, including scheduled runs of parameterized notebooks and collecting code and output into notebooks to create a record of various jobs. It also describes the extensive infrastructure required to support this at scale. At the time of its writing in 2018, the authors were planning to run more than 150,000 notebook-based jobs per day. This is the first of a two-part series. The second, Scheduling Notebooks at Netflix, dives more deeply into the details. Note that two of the original authors, Matthew Seal and Michelle Ufford have since left Netflix to found noteable.io, a “SaaS offering for integrated Jupyter Notebooks.” | How to share Jupyter Notebooks: complements the previous article by providing more ways to share JNs including gist, nbviewer, and binder. By Andrea Zonca | Presenting Code Using Jupyter Notebook Slides: I was today years old when I learned that the ability to run slide-based presentations was built into JNs! A very short tutorial on how to do it by Matthew Speck. | . My focus above is on Jupyter Notebooks, but there are lots of cool things going on in the RStudio world as well, especially related single-source, “one-click” publishing. Many great web-books about/using R have been published from R Notebooks using R markdown and bookdown, including Hadley Wickham’s classic R for Data Science. A list of other examples can be found on the bookdown archive page. . Here are a few relevant and timely R-related links: . RStudio 1.4 Preview: Citations: a close look at the deep support for citations included in the upcoming RStudio v1.4, currently available as a preview release. Other features in the update include improved support for Python and a visual markdown editor. | Single-source publishing for R users: describes a typical process for generating html- and pdf-based books from R Markdown using bookdown and ways to simplify and automate the process. I don’t pretend to understand everything going on here, but know enough to appreciate the author, Maëlle Salmon’s work enough to share and bookmark it! | .",
            "url": "https://olearydj.github.io/antisimplistic/worthreading/jupyter/2020/11/19/wr-exploring-jn-based-research.html",
            "relUrl": "/worthreading/jupyter/2020/11/19/wr-exploring-jn-based-research.html",
            "date": " • Nov 19, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Worth Reading, November 16, 2020",
            "content": "This first installment is unusually long as I wanted to include material going back a few weeks. . Web Articles . How to improve software engineering skills as a researcher: another excellent article by Lj Miranda, which provides a roadmap of skills that begins with version control and culminates in the deployment of a ML service. This is presented in the context of introducing those in academics / research to software engineering fundamentals. It is not a tutorial, but provides a number of such links for each step in the journey, many of which I will likely share in future editions of Worth Reading. | Balanced Accuracy: What and Why?: Jo Etzel discusses the challenges with interpreting overall classification accuracy (the proportion of examples correctly classified) when the training set is imbalanced, and suggests that balanced accuracy (the average of the proportion corrects of each class individually) may be more useful in those situations. See also the papers related to balanced accuracy, below. | The AI Hierarchy of Needs: summons Maslow to make the point that good data science built on a hierarchy of methods (house of cards?) starting with collection, and moving through storing, exploring, and labeling, before arriving at learning. The success at each level depends on the quality of work in those before. Written by Monica Rogati, former VP of Data at Jawbone and Senior Data Scientist at LinkedIn. | Data Science Archetypes: part of the Navigating a Data Science Career unit in Brandon Rohrer’s End-to-End Machine Learning online course. In it, Brandon identifies 6 archetypes of people in data science (generalist, detective, oracle, maker, unicorn, and diva) based on their mix of skills in analysis, modeling, engineering, and mechanics. I included this useful perspective in my recent talk on AI, ML, and Data Science Concepts for the Data Science Society of Auburn. | Making Peace with Personal Branding: valuable tips from Rachel Thomas covering various ways to build a personal brand, including blogging, twitter, and public speaking. Rachel is the co-founder of fastai, which I talk a lot about in the first item on this list. I’ve really enjoyed everything I’ve read by her, including this interview on Hacker Noon. | Could web-based development (game) engines be the end of App Stores?: a quick summary of the history of the app store leading up to recent controversy concerning Apple’s app store policies and pricing, including their very public legal battle with Epic Games. Also discusses the “rise of the game engines,” and the movement towards web-based tools in this space, including the author, Alan Smithson’s, own MetaVRse Engine. Particularly timely given this morning’s announcement from Apple that, starting January 1, 2021, they are cutting app store fees by 50% (to 15%) for developers / businesses earning less than $1m/yr. | . Online Education / Tutorials . Supervised Machine Learning Case Studies in R: an excellent overview of and introduction to supervised learning the tidyverse / tidymodels way by Julia Silge. Formerly a Data Scientist at Stack Overflow, Julia is now a Software Engineer at RStudio where she maintains tidymodels. A mix of slides and simple interactive code exercises that walk you through 4 different small but comprehensive case studies. | Intro to tidymodels with nflfastR: this slide show from Tom Mock, Customer Success Representative in the high tech and sports vertical at RStudio, is a nice follow-on to his colleague’s work on the previous item. Covers an end-to-end case study of tidymodel methods using NFL data in a 90 minute workshop format. Tom also runs the popular #TidyTuesday R project community. | Exponential Smoothing: a great summary of basic smoothing and time series methods, cleanly implemented with modern R methods. This page is part of what appears to be an excellent resource provided by the University of Cincinnati, the UC Business Analytics R Programming guide, originally developed by Bradley Boehmke. | . Papers . Experimenting with Automatic Video Creation from a Web Page: an interesting summary of a paper from Google Research. As the title suggests, they have developed a method to automatically convert static web pages into short marketing videos, given duration and aspect ratio. | Data Organization in Spreadsheets: provides sensible recommendations for structuring, filling out, and using spreadsheets for storing and organizing data. Written by Karl Browman and Kara Woo | PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison: “introduces the Penn Machine Learning Benchmark (PMLB), a publicly available dataset initialized with 165 real-world, simulated, and toy benchmark datasets for evaluating supervised classification methods.” A Python interface is provided to facilitate loading and preprocessing the data. A wide range of scikit-learn classifiers were evaluated to assess performance. | .",
            "url": "https://olearydj.github.io/antisimplistic/worthreading/2020/11/16/readings.html",
            "relUrl": "/worthreading/2020/11/16/readings.html",
            "date": " • Nov 16, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Big Picture Data Science Concepts",
            "content": "In less than 4 months, with minimal fanfare, the Data Science Society of Auburn has attracted nearly 100 members, including a mix of graduate and undergraduate students pursuing a wide range of degrees. Upon hearing about this new student organization I reached out to the founder, Jordan Eckert, to offer my assistance. I have since been elected as the Director of Career Programming. If you are reading this and are interested in presenting to our students and/or participating in an upcoming career event, I welcome your email at dan.oleary@auburn.edu! . Today I presented a talk intended to help students understand the “big picture” of Data Science and connect it to study / career opportunities. It was inspired during a lecture on neural networks, when a classmate asked ‘how is this related to AI?’ Most classes are so busy with the details that they fail to explain the big picture. I hope this helps close that gap. . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it on GitHub. .",
            "url": "https://olearydj.github.io/antisimplistic/presentation/datascience/2020/11/16/dssa-aiml-concepts.html",
            "relUrl": "/presentation/datascience/2020/11/16/dssa-aiml-concepts.html",
            "date": " • Nov 16, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "BET 3520 Fall 2020 Course Summary",
            "content": "For the last two years I’ve supported first-year courses in Auburn’s Business-Engineering-Technology (BET) minor as a teaching assistant and occasional lecturer. This Fall I was given the opportunity to take over those courses as the instructor of record. 26 lectures, 850+ new slides, and more assignments / grading than I’d like to think about later, my inaugural semester for BET 3520 and 3560 is all but over. . To celebrate I thought I’d share the following deck, a version of which was used to do the final exam review for 3520. It condenses over 400 slides worth of material, spanning more than 22 hours of traditional lectures (delivered via zoom, of course, #COVID), into about 70 slides that highlight about 30% of the semester’s total content. . An embedded PDF viewer should show up below, but it can be finicky. Give it a second or try refreshing. Otherwise, you can view / download it on GitHub. .",
            "url": "https://olearydj.github.io/antisimplistic/teaching/presentation/2020/11/13/bet3520-fall2020.html",
            "relUrl": "/teaching/presentation/2020/11/13/bet3520-fall2020.html",
            "date": " • Nov 13, 2020"
        }
        
    
  
    
  
    
        ,"post13": {
            "title": "Blogging with Fastpages",
            "content": "I may be a hypocrite, but I’m not wrong… . I’ve spent over 25 years preaching the value of a portfolio site to artists, software engineers, and designers that wanted to join my studio or learn how to get into games or related industries. Now that I’m 6 months into my PhD and planning for career 2.0, it’s past time for me to do the same. This sums up the justification in a single tweet: . The popularity of this tweet makes me laugh, because it&#39;s an illustrative example! I thought about trying to get a screenshot of David&#39;s slide, but just took a terrible phone photo of a webinar and tweeted with the quote. . &mdash; Amelia McNamara (@AmeliaMN) July 22, 2018 I recently finished reading Build a Career in Data Science1, which I recommend. It motivated me to start the process of sharing my work. . Introduction to Fastpages . This site is built with fastpages, a platform for building static Jekyll web blogs from Jupyter Notebooks (JNs). Markdown and Word DOCs are also supported. . . From their github page: . fastpages automates the process of creating blog posts via GitHub Actions, so you don’t have to fuss with conversion scripts. A full list of features can be found on GitHub. . Fastpages is built by the team that created the deep learning PyTorch front-end fastai and the Python programming environment nbdev. The team is led by Jeremy Howard, former President and Chief Data Scientist of Kaggle and author of Deep Learning for Coders with Fastai and PyTorch: AI Applications Without a PhD2. . I was first introduced fastpages and nbdev during the recent ACM TechTalk, It’s Time Deep Learning Learned from Software Engineering, presented by Howard and moderated by his collaborator Hamel Husain. . From Introducing Fastpages: . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . Some resources I found helpful in launching this blog: . The fastpages github page has detailed setup instructions | This video tutorial created by Abdul Majed walks you through the initial setup process | A sample jupyter-based blog page provides a live demonstration of its capabilities | . I was able to get the initial site live quite quickly with little drama, and have found the process of modifying existing content and publishing new material straightforward so far. Before choosing fastpages I considered the following hosting / authoring options: . A general purpose site like Wix | A page on Medium or Substack | Other JN-based tools like Pelican | Finding a way to use JNs in blogdown3 | . I decided that Fastpages was the best way for me to publish JN-based work directly, with professional look and feel, on a site that I control, for free. That’s a pretty great combination. I also like that it supports Markdown (like this page), is part of the larger fast.ai family of products (and philosophy), and is hosted on github. . Finally, if you still need to be convinced of the value of building a portfolio, and what to include in one give the following resources a look: . Advice to aspiring data scientists: start a blog on David Robinson’s Variance Explained | Data Science Portfolios That Will Get You the Job | Thinking of blogging about Data Science? Here are some tips and possible benefits. | . Many of us need to spend less time consuming, and more time creating. Now you know how and why, so go publish something! . . Robinson, Emily, and Jacqueline Nolis. Build a Career in Data Science. Simon and Schuster, 2020. &#8617; . | Howard, Jeremy, and Sylvain Gugger. Deep Learning for Coders with Fastai and PyTorch. O’Reilly Media, Inc., 2020. &#8617; . | There is a lot of cool stuff going on in R these days and blogdown supports a lot of interesting publishing workflows, but I ultimately decided that it would probably be easier to find a way to publish my R work using fastpages than to publish my Python work with blogdown. Time will tell… &#8617; . |",
            "url": "https://olearydj.github.io/antisimplistic/publication/jupyter/2020/11/06/starting-fastpages.html",
            "relUrl": "/publication/jupyter/2020/11/06/starting-fastpages.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Links . Blog / Portfolio | LinkedIn | Hybrid CV, Spring 2022 | . Bio . . I’m Dan O’Leary, aka Antisimplistic, a second career Instructor, PhD Student and Researcher in the Department of Industrial and Systems Engineering at Auburn University. After 23 years in the games industry as a co-founder of the independent development studio n-Space, it was past time for a change. . I currently teach undergraduate courses in Python / SQL and Product Development. The later is a 3-course sequence covering innovation, entrepreneurship, lean startup methods, and leadership in the department’s Business-Engingeering-Technology minor, offered by the Thomas Walter Center for Technology Management. I am also a member of the teaching team for three graduate-level classes in Product Development and Engineering Econ / Finance. . My academic and research interests are broad, encompassing data science, simulation, and visualization. For my dissertation I am exploring the efficacy of mixed reality training methods for workforce development in the manufacturing industry. . Until recently, I continued to support and maintain GUNSTRUCTION, which I founded in 2012. From 2020 to 2021 it logged nearly 200 million user interactions across web, iOS, and Android devices. In order to focus on my transition to academics, GUNSTRUCTION was sold in the summer of 2021. .",
          "url": "https://olearydj.github.io/antisimplistic/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://olearydj.github.io/antisimplistic/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}